defaults:
  - _self_
hydra:
  run:
    dir: ${log_dir}
_target_: src.agent.eval.EvalAgent

log_dir: ${oc.env:VLA_LOG_DIR}/eval_bridge/${name}_${seed}/${env.task}_${now:%H-%M-%S}
name:
device: cuda
seed: 42
resume_checkpoint_path:
n_eval_episode: 72     # octo simpler runs 3 seeds with 24 configs each
n_video:  ${n_eval_episode}
# sweeps:
#   urdf_version:
#     - null

env:
  task:
  adapter:
    _target_: src.agent.env_adapter.simpler.BridgeSimplerAdapter
    dataset_statistics_path: ${oc.env:VLA_DATA_DIR}/resize_224/bridge_dataset/1.0.0/dataset_statistics_02e49fbdf286cd56a8167823626895770c4a139f0b191f36418753d11dd5e48c.json
    pretrained_model_path: ${oc.env:TRANSFORMERS_CACHE}/paligemma-3b-pt-224
    tokenizer_padding: max_length
    max_seq_len: 276  # fixed 256 for image + max 20 for text
    num_image_tokens: 256
    image_size: [224, 224]

flow_schedule: gamma
num_inference_steps: 10
final_action_clip_value: 1.0  # data normalized in [-1,1]

cond_steps: 1
horizon_steps: 4
act_steps: 4
action_dim: 7 # EEF_POS
proprio_dim: 7  # POS_EULER

image_text_hidden_size: 2048  # gemma
proprio_hidden_size: 512
action_hidden_size: 1024
time_hidden_size: 256 # if using adaptive
action_expert_adaptive_mode:
use_fourier_features: False

# Fixed
image_token_index: 257152
vocab_size: 257216
pad_token_id: 0

vision:
  _target_: src.model.paligemma.siglip.SiglipVisionModel
  config:
    hidden_size: 1152 # siglip
    intermediate_size: 4304
    num_hidden_layers: 27
    num_attention_heads: 16
    num_channels: 3
    image_size: 224
    patch_size: 14
    layer_norm_eps: 1e-6
    attention_dropout: 0.0
    num_image_tokens: 256

vision_projector:
  _target_: src.model.paligemma.siglip.PaliGemmaMultiModalProjector
  config:
    vision_config:
      hidden_size: 1152
      projection_dim: ${image_text_hidden_size}

joint:
  _target_: src.model.vla.modules.JointModel
  config:
    action_expert_adaptive_mode: ${action_expert_adaptive_mode}
    time_hidden_size: ${time_hidden_size}
    hidden_sizes:
      - ${image_text_hidden_size}
      - ${proprio_hidden_size}
      - ${action_hidden_size}
    intermediate_sizes: # mlp hidden size
      - 16384 # gemma
      - 1024
      - 4096
    #
    num_hidden_layers: 18
    num_attention_heads: 8
    num_key_value_heads: 1
    head_dim: 256
    max_position_embeddings: 8192
    rms_norm_eps: 1e-6
    rope_theta: 10000.0
    attention_bias: False
    attention_dropout: 0.0
    pad_token_id: ${pad_token_id}
